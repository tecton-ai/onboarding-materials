{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tecton-jon/onboarding-materials/blob/main/Tecton_Rift_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Lab: Productionizing Real-Time Features with Tecton and Rift\n",
        "\n",
        "In this lab, we will explore how we can develop and test real-time features for a fraud detecton use case using Tecton and Rift.\n",
        "\n",
        "Rift is Tecton's Python-first compute engine for efficiently computing batch, stream, and real-time features using Python and SQL. With Rift we can develop and test features locally in any Python environment and then productionize with a single step.\n",
        "\n",
        "Let's try it out!"
      ],
      "metadata": {
        "id": "SlsggVTehkiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Install Pre-Reqs\n",
        "\n",
        "Run the following commands to install Tecton and other pre-requisites.\n",
        "\n",
        "**After installation, be sure to restart your session via \"Runtime -> Restart Session\" in the menu above.**"
      ],
      "metadata": {
        "id": "pFUbSoc8hEr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv tecton\n",
        "!source tecton/bin/activate\n",
        "!pip install 'tecton[rift]' s3fs fsspec scikit-learn"
      ],
      "metadata": {
        "id": "FkL6Pc_MvE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Restart your session via \"Runtime -> Restart Session\"\n",
        "\n",
        "## üë©‚Äçüíª Log into a Tecton account"
      ],
      "metadata": {
        "id": "YQuFAJBGjbi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRf76TsNtk0x"
      },
      "outputs": [],
      "source": [
        "import tecton, os, requests, json\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from tecton import *\n",
        "from tecton.types import *\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "tecton.set_validation_mode('auto')\n",
        "\n",
        "tecton.login('lab.tecton.ai')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ca360037-6e9e-4fc8-a04f-2d033a3016b6",
          "showTitle": false,
          "title": ""
        },
        "id": "KdIySM8Ztk0y"
      },
      "source": [
        "## üîé Examine Raw Data\n",
        "\n",
        "On S3 we have a historical log of a transaction stream representing transactions that users made at different merchants in the last few years.\n",
        "\n",
        "We can use this data to brainstorm streaming features and even test them out with Tecton!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "70d8ea68-5713-4bb4-8223-f19e486909e5",
          "showTitle": false,
          "title": ""
        },
        "id": "CZtqdn_gtk0z"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"s3://tecton.ai.public/tutorials/fraud_demo/transactions/data.pq\", storage_options={'anon': True})\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "db1ea257-ee04-462e-af92-454d42820bec",
          "showTitle": false,
          "title": ""
        },
        "id": "bdjdca92tk00"
      },
      "source": [
        "## üåä Define and Test Streaming Features\n",
        "\n",
        "Streaming features can be tested offline in a notebook and used to train a model. Tecton uses the historical log of a stream to compute accurate historical feature values.\n",
        "\n",
        "‚úÖ Try extending the definition below with more features, such as:\n",
        "\n",
        "- The total dollar amount of transactions a user has made in the last 1 minute, 1 hour, and 1 year.\n",
        "- The total number of transactions a user has made in the last 1 minute, 1 hour, and 1 year.\n",
        "\n",
        "You may find [this documentation](https://docs.tecton.ai/docs/beta/defining-features/feature-views/aggregation-engine/aggregation-functions) helpful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b4a3ec27-3483-4358-b830-0ab2677ea9e8",
          "showTitle": false,
          "title": ""
        },
        "id": "5k6iqWl1tk00"
      },
      "outputs": [],
      "source": [
        "# Define a stream source, including the historical log of the stream\n",
        "transactions_stream = StreamSource(\n",
        "    name='transactions_stream',\n",
        "    stream_config=PushConfig(),\n",
        "    batch_config=FileConfig(\n",
        "        uri='s3://tecton.ai.public/tutorials/fraud_demo/transactions/data.pq',\n",
        "        file_format='parquet',\n",
        "        timestamp_field='timestamp'\n",
        "    ),\n",
        "    schema=[Field('user_id', String), Field('timestamp', Timestamp), Field('amt', Float64)]\n",
        ")\n",
        "\n",
        "# Define the entity we are creating features for\n",
        "user = Entity(name='user', join_keys=['user_id'])\n",
        "\n",
        "# Define features\n",
        "@stream_feature_view(\n",
        "    source=transactions_stream,\n",
        "    entities=[user],\n",
        "    mode='pandas',\n",
        "    aggregations=[\n",
        "        Aggregation(function='mean', column='amt', time_window=timedelta(minutes=1)),\n",
        "        Aggregation(function='mean', column='amt', time_window=timedelta(minutes=5)),\n",
        "        Aggregation(function='mean', column='amt', time_window=timedelta(days=365))\n",
        "    ],\n",
        "    schema=[Field(\"user_id\", String), Field(\"timestamp\", Timestamp), Field(\"amt\", Float64)]\n",
        ")\n",
        "def user_transaction_features(transactions):\n",
        "    return transactions[['user_id', 'timestamp', 'amt']]\n",
        "\n",
        "\n",
        "# Compute features\n",
        "start = datetime(2023,1,1)\n",
        "end = datetime(2023,6,1)\n",
        "\n",
        "feature_df = user_transaction_features.get_historical_features(start_time=start, end_time=end).to_pandas()\n",
        "\n",
        "display(feature_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è±Ô∏è Define and Test Real-Time Features\n",
        "\n",
        "Now let's define define a feature that checks if the current transaction amount a user is seeking to make is higher than their historical average.\n",
        "\n",
        "Because this feature depends on real-time info (the current transaction amount), we need to compute it at the time of the request. That's exactly where on-demand features come in.\n",
        "\n",
        "‚úÖ Try changing the definition below to compare the transaction to the 1 year average instead of the 5 minute average."
      ],
      "metadata": {
        "id": "rhqBa92Nep2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define on-demand features\n",
        "transaction_request = RequestSource(schema=[Field(\"amt\", Float64)])\n",
        "\n",
        "@on_demand_feature_view(\n",
        "    sources=[transaction_request, user_transaction_features],\n",
        "    mode=\"python\",\n",
        "    schema=[Field(\"transaction_amount_is_higher_than_average\", Bool)],\n",
        ")\n",
        "def transaction_amount_is_higher_than_average(transaction_request, user_transaction_features):\n",
        "    amount_mean = user_transaction_features[\"amt_mean_5m_continuous\"]\n",
        "    amount_mean = 0 if amount_mean is None else amount_mean\n",
        "    return {\"transaction_amount_is_higher_than_average\": transaction_request[\"amt\"] > amount_mean}\n",
        "\n",
        "\n",
        "# Test on-demand features\n",
        "averages = feature_df.drop(columns=['user_id', 'timestamp', '_effective_timestamp']).iloc[0].to_dict()\n",
        "request = {'amt': 10.4}\n",
        "features = transaction_amount_is_higher_than_average.run(user_transaction_features=averages, transaction_request=request)\n",
        "\n",
        "print('\\nRequest amount: ' + str(request['amt']))\n",
        "print('Average: ' + str(averages['amt_mean_5m_continuous']))\n",
        "print(str(features))"
      ],
      "metadata": {
        "id": "ojMVzfG26nRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4367e3a5-0942-42d9-81b4-9221cd8b2d97",
          "showTitle": false,
          "title": ""
        },
        "id": "2MH1xogXtk00"
      },
      "source": [
        "## üßÆ Generate Training Data\n",
        "\n",
        "Now that we've created some features, it's time to join them into a training data set so we can train a model.\n",
        "\n",
        "First let's load up a list of historical training events. These events represent labeled historical user transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "3037c380-6a48-4ad6-93af-0e2ae369b9b7",
          "showTitle": false,
          "title": ""
        },
        "id": "JEY5-btgtk01"
      },
      "outputs": [],
      "source": [
        "training_events = pd.read_parquet(\"s3://tecton.ai.public/tutorials/fraud_demo/transactions/data.pq\", storage_options={'anon': True}) \\\n",
        "                    [['user_id', 'timestamp', 'amt', 'is_fraud']]\n",
        "\n",
        "display(training_events)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our training events, we can get features for those events by adding them to a Feature Service and calling `get_historical_features(events)`.\n",
        "\n",
        "The feature service defines the set of features we want to serve to our model offline and online."
      ],
      "metadata": {
        "id": "3c9EhlkHpjlB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6cb99278-9306-41f4-9579-d49558bf6546",
          "showTitle": false,
          "title": ""
        },
        "id": "ybav_BHTtk01"
      },
      "outputs": [],
      "source": [
        "from tecton import FeatureService\n",
        "\n",
        "fraud_detection_feature_service = FeatureService(\n",
        "    name=\"fraud_detection_feature_service\",\n",
        "    features=[user_transaction_features, transaction_amount_is_higher_than_average]\n",
        ")\n",
        "\n",
        "training_data = fraud_detection_feature_service.get_historical_features(training_events).to_pandas()\n",
        "\n",
        "display(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Train a Model\n",
        "\n",
        "With a training dataset full of features, we can now train a simple logistic regression model to detect fraudulent transactions."
      ],
      "metadata": {
        "id": "Cuzkl3dQisp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "df = training_data.drop(['user_id', 'timestamp', 'amt'], axis=1)\n",
        "\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "num_cols = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
        "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "num_pipe = make_pipeline(\n",
        "    SimpleImputer(strategy='median'),\n",
        "    StandardScaler()\n",
        ")\n",
        "\n",
        "cat_pipe = make_pipeline(\n",
        "    SimpleImputer(strategy='constant', fill_value='N/A'),\n",
        "    OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        ")\n",
        "\n",
        "full_pipe = ColumnTransformer([\n",
        "    ('num', num_pipe, num_cols),\n",
        "    ('cat', cat_pipe, cat_cols)\n",
        "])\n",
        "\n",
        "model = make_pipeline(full_pipe, LogisticRegression(max_iter=1000, random_state=42))\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_predict = model.predict(X_test)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "rPdMChnoDayD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0xnpaWbtk01"
      },
      "source": [
        "## üöÄ Apply Features to Production\n",
        "\n",
        "**NOTE: This step has been done for you already.**\n",
        "\n",
        "Productionizing features with Tecton is easy. Simply paste the definitions into a repo of Python files, select a workspace, and run `tecton apply to productize\n",
        "\n",
        "Create a feature repo:\n",
        "```bash\n",
        "mkdir feature-repo && cd feature-repo\n",
        "tecton init\n",
        "touch features.py\n",
        "```\n",
        "\n",
        "Apply features to production:\n",
        "```bash\n",
        "tecton login lab.tecton.ai\n",
        "tecton workspace select prod\n",
        "tecton apply\n",
        "```\n",
        "\n",
        "You can check out the applied features in Tecton's web UI [here](https://lab.tecton.ai/app/repo/prod/features).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm-D0A-Ptk01"
      },
      "source": [
        "## ‚ö°Ô∏è Ingest Streaming Events and Read Real-Time Features\n",
        "\n",
        "Once we've productionized our Stream Source, we can start sending events to it. Any features defined against this source will be updated in real time!\n",
        "\n",
        "Try adding your own name as the `user_id` below and watch how feature values update immediately."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tecton.set_credentials(tecton_api_key='3fcbbfb66c3c4d1a7ce1b9e02f410a1f')\n",
        "os.environ['TECTON_API_KEY'] = '3fcbbfb66c3c4d1a7ce1b9e02f410a1f'"
      ],
      "metadata": {
        "id": "5e44pRLtUt56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOqfHswztk01"
      },
      "outputs": [],
      "source": [
        "ws = tecton.get_workspace('prod')\n",
        "registered_transactions_stream = ws.get_data_source('transactions_stream')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGAYs7vatk01"
      },
      "outputs": [],
      "source": [
        "registered_transactions_stream.ingest({\n",
        "    'user_id': 'matt-lab',\n",
        "    'timestamp': datetime.utcnow(),\n",
        "    'amt': 100.00\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRDcCdOetk01"
      },
      "outputs": [],
      "source": [
        "fs = ws.get_feature_service('fraud_detection_feature_service')\n",
        "features = fs.get_online_features(join_keys={'user_id': 'matt-lab'}, request_data={'amt': 50}).to_dict()\n",
        "\n",
        "pprint(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî• Define Online Prediction Pipeline\n",
        "\n",
        "Now that we have online feature values, we can create a prediction pipeline to determine if a transaction is fraudulent and whether we should accept or reject it.\n",
        "\n",
        "To do this we will define three functions to:\n",
        "\n",
        "1. Get features from Tecton\n",
        "2. Use the real-time features to make a prediction with the model\n",
        "3. Use the model prediction to accept or reject a transaction"
      ],
      "metadata": {
        "id": "Qs4j0u72GjjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get features from Tecton\n",
        "def get_online_feature_data(user_id, amt):\n",
        "    headers = {\"Authorization\": \"Tecton-key \" + os.environ['TECTON_API_KEY']}\n",
        "\n",
        "    request_data = f'''{{\n",
        "        \"params\": {{\n",
        "            \"feature_service_name\": \"fraud_detection_feature_service\",\n",
        "            \"join_key_map\": {{\"user_id\": \"{user_id}\"}},\n",
        "            \"metadata_options\": {{\"include_names\": true}},\n",
        "            \"request_context_map\": {{\"amt\": {amt}}},\n",
        "            \"workspace_name\": \"prod\"\n",
        "        }}\n",
        "    }}'''\n",
        "\n",
        "    online_feature_data = requests.request(\n",
        "        method=\"POST\",\n",
        "        headers=headers,\n",
        "        url=\"https://lab.tecton.ai/api/v1/feature-service/get-features\",\n",
        "        data=request_data,\n",
        "    )\n",
        "\n",
        "    online_feature_data_json = json.loads(online_feature_data.text)\n",
        "\n",
        "    return online_feature_data_json\n",
        "\n",
        "# Use the real-time features to make a prediction with the model\n",
        "def get_prediction_from_model(feature_data):\n",
        "    columns = [f[\"name\"].replace(\".\", \"__\") for f in feature_data[\"metadata\"][\"features\"]]\n",
        "    data = [feature_data[\"result\"][\"features\"]]\n",
        "\n",
        "    features = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    return model.predict(features)[0]\n",
        "\n",
        "# Use the model prediction to accept or reject a transaction\n",
        "def evaluate_transaction(user_id, amt):\n",
        "    online_feature_data = get_online_feature_data(user_id, amt)\n",
        "    is_predicted_fraud = get_prediction_from_model(online_feature_data)\n",
        "\n",
        "    print('Features: ' + str(online_feature_data[\"result\"][\"features\"]))\n",
        "    print('Model Score: ' + str(is_predicted_fraud))\n",
        "\n",
        "    if is_predicted_fraud == 0:\n",
        "        print('Transaction accepted.')\n",
        "    else:\n",
        "        print('Transaction denied.')"
      ],
      "metadata": {
        "id": "DsoofOr-NpRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚≠êÔ∏è Evaluate Transactions in Real-Time\n",
        "\n",
        "Now we have a single decision API to evaluate transactions in real-time!\n",
        "\n",
        "Let's test it out."
      ],
      "metadata": {
        "id": "s6cJ46vhHSyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_transaction('matt', 182.46)"
      ],
      "metadata": {
        "id": "7LP2M-xlFiJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_VrAwPRSQHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "Tecton Demo",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "unified-tecton",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}